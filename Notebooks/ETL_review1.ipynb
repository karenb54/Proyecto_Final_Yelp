{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6c0ceea-16a9-4674-b4b8-02a4de5cbfb1",
   "metadata": {},
   "source": [
    "# Transformación a Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d033368-0a0c-43d3-9c8d-01c9d49572fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "#import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae09d587-ff81-4fab-9e15-08fa4d9fefa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/05 14:09:56 WARN Utils: Your hostname, hugo-81we resolves to a loopback address: 127.0.1.1; using 10.201.213.244 instead (on interface wlp0s20f3)\n",
      "24/11/05 14:09:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/05 14:09:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/05 14:09:56 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "24/11/05 14:09:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "#Crear sesión de spark\n",
    "spk = SparkSession.builder \\\n",
    "    .config(\"spark.driver.port\", \"4040\") \\\n",
    "    .config(\"spark.local.dir\", \"/run/media/hugo/06368a0d-700b-4ac5-9159-173c295dcaed/\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"file:///run/media/hugo/06368a0d-700b-4ac5-9159-173c295dcaed/\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"EDAReview\") \\\n",
    "    .getOrCreate()\n",
    "spk.sparkContext.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58832f7d-ed57-46d5-8b44-8b56442f68c4",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18967207-258b-444a-95f8-694aa683a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('./reviews-estados/**/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37c8afb4-d858-44df-82d9-c0f6a921cbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spk.read.json(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e48ea3d-62bc-4e44-9997-96318babe941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Leer y cargar los dataset en un dataframe spark\n",
    "df = spk.read.parquet('file:///run/media/hugo/06368a0d-700b-4ac5-9159-173c295dcaed/Google/review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f667af58-b6b8-4b03-adf9-cdb8592a1fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gmap_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- pics: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- url: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |-- rating: long (nullable = true)\n",
      " |-- resp: struct (nullable = true)\n",
      " |    |-- text: string (nullable = true)\n",
      " |    |-- time: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- time: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Imprimir esquema de datos\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71a998ca-b3d3-4016-b3b6-1b8506e93194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "89946359"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Contar cantidad de filas del df\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30c19fb2-e027-42d4-aa89-456e13ea2585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------------+------+----+--------------------+-------------+--------------------+\n",
      "|             gmap_id|          name|                pics|rating|resp|                text|         time|             user_id|\n",
      "+--------------------+--------------+--------------------+------+----+--------------------+-------------+--------------------+\n",
      "|0x8752f569a04d64b...|     Mc Family|[{[https://lh5.go...|     5|NULL|I owe the success...|1614273754022|11356053341921870...|\n",
      "|0x87528440b7ee298...|Sherri Narvaez|[{[https://lh5.go...|     1|NULL|See post from Mid...|1493078507343|10134158574946813...|\n",
      "|0x874d907b753aa79...| Sarah M. Maya|[{[https://lh5.go...|     5|NULL|Was extremely gra...|1482624188733|10897238372292850...|\n",
      "|0x87528a24ee73e5b...|     Tania Nay|[{[https://lh5.go...|     5|NULL|                NULL|1527482637204|11193326059038268...|\n",
      "|0x80ca6ccb4e4a34f...|  Jacob Colvin|[{[https://lh5.go...|     5|NULL|Great building to...|1550358870272|11645559622322002...|\n",
      "+--------------------+--------------+--------------------+------+----+--------------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtrar no Nulos de 'pics' para ver contenido\n",
    "df.filter(df['pics'].isNotNull()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5c86f05-b2ca-4ea0-a7b6-b1c894a74751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+------+--------------------+--------------------+-------------+--------------------+\n",
      "|             gmap_id|            name|rating|                resp|                text|         time|             user_id|\n",
      "+--------------------+----------------+------+--------------------+--------------------+-------------+--------------------+\n",
      "|0x87528767d0ec0e4...|      Liz W Poch|     5|                NULL|Paige is the best...|1627085008811|11118259507767436...|\n",
      "|0x87528767d0ec0e4...|Mario D'Ambrosio|     1|                NULL|I have updated my...|1626437133578|11768917084644824...|\n",
      "|0x87528767d0ec0e4...| Kathleen Hambly|     5|                NULL|Simone is the bes...|1585165423975|11583468565499948...|\n",
      "|0x87528767d0ec0e4...|   Crystal Olsen|     1|                NULL|Ive called severa...|1591629458998|11267961121373171...|\n",
      "|0x87528767d0ec0e4...|    Sarah Jensen|     5|{Thank you for yo...|My fiance and I h...|1510264647735|10859942935026242...|\n",
      "+--------------------+----------------+------+--------------------+--------------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Solo contiene las urls de los locales, se eliminará esta columna\n",
    "df = df.drop('pics')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e9559f9-f1b0-4584-98ca-7fd0f7813dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+------+--------------------+--------------------+-------------+--------------------+-------------------+\n",
      "|             gmap_id|            name|rating|                resp|                text|         time|             user_id|              fecha|\n",
      "+--------------------+----------------+------+--------------------+--------------------+-------------+--------------------+-------------------+\n",
      "|0x87528767d0ec0e4...|      Liz W Poch|     5|                NULL|Paige is the best...|1627085008811|11118259507767436...|2021-07-23 21:03:28|\n",
      "|0x87528767d0ec0e4...|Mario D'Ambrosio|     1|                NULL|I have updated my...|1626437133578|11768917084644824...|2021-07-16 09:05:33|\n",
      "|0x87528767d0ec0e4...| Kathleen Hambly|     5|                NULL|Simone is the bes...|1585165423975|11583468565499948...|2020-03-25 16:43:43|\n",
      "|0x87528767d0ec0e4...|   Crystal Olsen|     1|                NULL|Ive called severa...|1591629458998|11267961121373171...|2020-06-08 12:17:38|\n",
      "|0x87528767d0ec0e4...|    Sarah Jensen|     5|{Thank you for yo...|My fiance and I h...|1510264647735|10859942935026242...|2017-11-09 18:57:27|\n",
      "+--------------------+----------------+------+--------------------+--------------------+-------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Convertir la columna de milisegundos a fecha\n",
    "df = df.withColumn('fecha', F.from_unixtime(F.col('time') / 1000).cast('timestamp'))\n",
    "\n",
    "# Mostrar el DataFrame actualizado\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b1c9147-3e30-44d4-9b50-dfec3e10eb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertir a minúsculas la columna de texto de review para facilitar la búsqueda\n",
    "df = df.withColumn('text', F.lower(F.col('text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c737cbcd-9295-4400-8d73-1c7a9fd04446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+------+--------------------+--------------------+-------------+--------------------+-------------------+----+\n",
      "|             gmap_id|            name|rating|                resp|                text|         time|             user_id|              fecha| año|\n",
      "+--------------------+----------------+------+--------------------+--------------------+-------------+--------------------+-------------------+----+\n",
      "|0x87528767d0ec0e4...|      Liz W Poch|     5|                NULL|paige is the best...|1627085008811|11118259507767436...|2021-07-23 21:03:28|2021|\n",
      "|0x87528767d0ec0e4...|Mario D'Ambrosio|     1|                NULL|i have updated my...|1626437133578|11768917084644824...|2021-07-16 09:05:33|2021|\n",
      "|0x87528767d0ec0e4...| Kathleen Hambly|     5|                NULL|simone is the bes...|1585165423975|11583468565499948...|2020-03-25 16:43:43|2020|\n",
      "|0x87528767d0ec0e4...|   Crystal Olsen|     1|                NULL|ive called severa...|1591629458998|11267961121373171...|2020-06-08 12:17:38|2020|\n",
      "|0x87528767d0ec0e4...|    Sarah Jensen|     5|{Thank you for yo...|my fiance and i h...|1510264647735|10859942935026242...|2017-11-09 18:57:27|2017|\n",
      "+--------------------+----------------+------+--------------------+--------------------+-------------+--------------------+-------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Crear una columna de año para poder agrupar después\n",
    "df = df.withColumn('año', F.year(F.col('fecha')))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33be882f-3a59-4857-9dcf-bbee18fb2752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df['user_id'].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11e99878-0856-4bdd-ba1f-e7ccdcc20d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Comprobar nulos en la columna de fecha\n",
    "df.filter(df['time'].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38d39917-0ae9-4fb1-b537-0eaf6e11fa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39307744"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Comprobar nulos en la columna de texto de review\n",
    "df.filter(df['text'].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05ba06ce-5746-429b-8978-3f36e0d1d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna({'text': 'nd'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79792e23-447f-4d01-b79a-4f7097eb7270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear columna id_rev combinando los datos de user_id y time\n",
    "df = df.withColumn('id_rev', F.concat(F.col('user_id'),F.lit('_'), F.col('time')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7590827-5830-4875-882a-50181a68bdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creando df de los datos agrupados por id_rev que aparecen más de una vez\n",
    "df_duplicados = df.groupBy('id_rev').count().filter(F.col('count') > 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46f6645a-5743-4858-bcb6-db5eea641f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Obtener una muestra de 5 datos de la columna 'id_rev' del df de duplicados\n",
    "muestras = df_duplicados.select(\"id_rev\").rdd.map(lambda row: row[0]).takeSample(False, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f1c2bf1-ed3e-42c8-9157-c6a40ff4dbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:=====================================================>  (49 + 2) / 51]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+------+----+--------------------+-------------+--------------------+-------------------+----+--------------------+\n",
      "|             gmap_id|         name|rating|resp|                text|         time|             user_id|              fecha| año|              id_rev|\n",
      "+--------------------+-------------+------+----+--------------------+-------------+--------------------+-------------------+----+--------------------+\n",
      "|0x8814ca00028f51d...|     Hai Tran|     4|NULL|               goods|1502148317389|11543514572841014...|2017-08-07 20:25:17|2017|11543514572841014...|\n",
      "|0x8814ca00028f51d...|     Hai Tran|     4|NULL|               goods|1502148317389|11543514572841014...|2017-08-07 20:25:17|2017|11543514572841014...|\n",
      "|0x87cd266c656b682...|Charles Stitz|     5|NULL|everything is gre...|1562273141132|10777154252087485...|2019-07-04 17:45:41|2019|10777154252087485...|\n",
      "|0x87cd266c656b682...|Charles Stitz|     5|NULL|everything is gre...|1562273141132|10777154252087485...|2019-07-04 17:45:41|2019|10777154252087485...|\n",
      "|0x883e49d0f2232b5...|   Layne byer|     5|NULL|best salvage yard...|1491847652775|11627635599752366...|2017-04-10 15:07:32|2017|11627635599752366...|\n",
      "|0x883e49d0f2232b5...|   Layne byer|     5|NULL|best salvage yard...|1491847652775|11627635599752366...|2017-04-10 15:07:32|2017|11627635599752366...|\n",
      "|0x866509fb08d8e3a...|  josh vargas|     5|NULL|            peaceful|1611957767727|10564322715806862...|2021-01-29 19:02:47|2021|10564322715806862...|\n",
      "|0x866509fb08d8e3a...|  josh vargas|     5|NULL|            peaceful|1611957767727|10564322715806862...|2021-01-29 19:02:47|2021|10564322715806862...|\n",
      "|0x54c8d50efa883f1...|     Jeff Ray|     4|NULL|i mean, it's a mc...|1542418494367|10754552903258224...|2018-11-16 22:34:54|2018|10754552903258224...|\n",
      "|0x54c8d50efa883f1...|     Jeff Ray|     4|NULL|i mean, it's a mc...|1542418494367|10754552903258224...|2018-11-16 22:34:54|2018|10754552903258224...|\n",
      "+--------------------+-------------+------+----+--------------------+-------------+--------------------+-------------------+----+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filtrar el DataFrame para controlar los datos duplicados y comprobar que sean duplicados\n",
    "df_muestras = df.filter(df['id_rev'].isin(muestras))\n",
    "\n",
    "# Mostrar el DataFrame filtrado\n",
    "df_muestras.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50d161e0-a027-4689-8512-b46dafac4f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminar Duplicados\n",
    "df = df.dropDuplicates([\"id_rev\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcb98eec-e5e1-4969-967c-815bb6883284",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Escribir nuevos archivos parquet con datos almacenados\n",
    "df_muestras = None\n",
    "df_duplicados = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42643eea-19a4-46cd-86fc-129df6a194fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el DataFrame en 3 partes (ajusta las proporciones según lo necesites)\n",
    "df_parts = df.randomSplit([0.3, 0.3, 0.4], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b437cfd-1d76-4932-a02a-d7d050c1da2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:=======================================================>(75 + 1) / 76]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o132.parquet.\n: java.net.ConnectException: Call From hugo-81we/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Conexión rehusada; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1558)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1455)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)\n\tat jdk.proxy2/jdk.proxy2.$Proxy46.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:965)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat jdk.proxy2/jdk.proxy2.$Proxy47.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1739)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:120)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\nCaused by: java.net.ConnectException: Conexión rehusada\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Net.java:682)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:1060)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)\n\tat org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1502)\n\t... 62 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Guardar cada parte en un archivo Parquet separado\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(df_parts):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mpart\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreviewETL/part_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/pyspark/sql/readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/venv/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o132.parquet.\n: java.net.ConnectException: Call From hugo-81we/127.0.1.1 to localhost:9000 failed on connection exception: java.net.ConnectException: Conexión rehusada; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:502)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:486)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:913)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:828)\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1616)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1558)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1455)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)\n\tat jdk.proxy2/jdk.proxy2.$Proxy46.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:965)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat jdk.proxy2/jdk.proxy2.$Proxy47.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1739)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1753)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1750)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1765)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:120)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\nCaused by: java.net.ConnectException: Conexión rehusada\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Net.java:682)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:1060)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:711)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:833)\n\tat org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1677)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1502)\n\t... 62 more\n"
     ]
    }
   ],
   "source": [
    "# Guardar cada parte en un archivo Parquet separado\n",
    "for i, part in enumerate(df_parts):\n",
    "    part.write.parquet(f'reviewETL/part_{i}.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cac7a2e6-149f-47ab-821a-09c5e5e4f51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.option(\"compression\", \"snappy\").parquet('reviewETL')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
